# -*- coding: utf-8 -*-
"""XGBoost_Model_for_Target_Income_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cDwLIM4WDyJC5YrwI1NNsNXHtIaCnyKo
"""


# -*- coding: utf-8 -*-
"""
This script performs an end-to-end machine learning pipeline with improvements for higher scores:
1. Loads and renames data.
2. Cleans column names to ensure uniqueness.
3. Performs advanced feature engineering by combining previous methods with new financial ratios and aggregates.
4. **Introduces binary indicators for missing values.**
5. Identifies and selects features based on missing values and correlation.
6. Sets up a preprocessing pipeline using IterativeImputer and OneHotEncoder.
7. Optimizes hyperparameters for individual XGBoost, CatBoost, and RandomForest Regressors using Optuna.
8. Creates and trains a VotingRegressor ensemble using the best configurations from individual models.
9. Evaluates the ensemble model's performance on the test set.
10. Provides model explainability for the primary (XGBoost) model using SHAP values.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer, IterativeImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from collections import defaultdict
import pickle
import json
import shap
import optuna # Import Optuna for Bayesian Optimization
import joblib

# Install category_encoders if not already installed for Target Encoding
try:
    import category_encoders as ce
except ImportError:
    print("Installing category_encoders...")
    import sys
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "category_encoders"])
    import category_encoders as ce
    print("category_encoders installed.")


# --- 1. Load Dataset and Column Mapping ---
data_path = "data/Hackathon_bureau_data_400.csv"
mapping_path = "data/participant_col_mapping.csv"

try:
    df = pd.read_csv(data_path)
    col_map = pd.read_csv(mapping_path)
    print("‚úÖ Dataset Loaded")
    print("Shape:", df.shape)
    print("Head of data:\n", df.head())
except FileNotFoundError as e:
    print(f"Error loading files: {e}. Make sure '{data_path}' and '{mapping_path}' are in the correct directory.")
    exit()

# --- 2. Build Mapping Dictionary and Rename Columns ---
mapping_dict = dict(zip(col_map['column_name'], col_map['description']))
df_renamed = df.rename(columns=mapping_dict)

print("\n‚úÖ Columns after initial renaming:")
print(df_renamed.columns.tolist()[:80])
print("\nüîç Column Type Summary:")
print(df_renamed.dtypes.value_counts())

# --- 3. Ensure all columns are unique ---
def make_unique_columns(columns):
    seen = defaultdict(int)
    new_cols = []
    for col in columns:
        if seen[col] == 0:
            new_cols.append(col)
        else:
            new_cols.append(f"{col}_dup{seen[col]}")
        seen[col] += 1
    return new_cols

initial_num_cols = df_renamed.shape[1]
df_renamed.columns = make_unique_columns(df_renamed.columns)
final_num_cols = df_renamed.shape[1]

if initial_num_cols != final_num_cols:
    print(f"üõë Warning: Number of columns changed after making unique. Before: {initial_num_cols}, After: {final_num_cols}. This might indicate an issue with how duplicates were handled previously.")
assert not df_renamed.columns.duplicated().any(), "‚ö†Ô∏è Still has duplicate columns after unique renaming!"
print("‚úÖ All columns are now unique.")

# --- 4. Identify Target and Feature Columns and Apply Target Transformation ---
target_candidates = [col for col in df_renamed.columns if 'target' in col.lower() or 'income' in col.lower()]
print("\nüéØ Possible target columns:", target_candidates)

target_col = "target"

if target_col not in df_renamed.columns:
    print(f"Error: Target column '{target_col}' not found in the DataFrame. Please check the mapping or target name.")
    exit()

# Store original target for loan_to_income_proxy calculation later if needed before log transform
df_renamed['target_original_scale'] = df_renamed[target_col].copy()

# Apply log1p transformation to the target variable
# This is common for skewed target distributions to help the model learn better.
df_renamed[target_col] = np.log1p(df_renamed[target_col])
print(f"‚úÖ Target column '{target_col}' has been log1p transformed.")

# Identify numeric and object columns AFTER renaming and ensuring uniqueness
all_numeric_cols = df_renamed.select_dtypes(include=[np.number]).columns.tolist()
# Exclude target_col AND target_original_scale from numeric features
numeric_cols = [col for col in all_numeric_cols if col not in [target_col, 'target_original_scale']]
categorical_cols = df_renamed.select_dtypes(include=['object']).columns.tolist()

print(f"\nüî¢ Numeric Columns (excluding target and original target scale): {len(numeric_cols)}")
print(f"üî° Categorical Columns: {len(categorical_cols)}")

print("Examples of numeric columns:", numeric_cols[:5])
print("Examples of categorical columns:", categorical_cols[:5])

# --- 5. Feature Engineering (combining previous methods with new advanced features) ---
print("\n‚öôÔ∏è Performing Advanced Feature Engineering...")

# --- Missingness Indicators ---
# Create binary flags for all columns that have missing values
# Do this before any imputation or feature engineering that might fill NaNs.
print("  - Creating missingness indicator features...")
initial_feature_set = numeric_cols + categorical_cols # Consider all features that will be used
missing_indicator_cols = []
for col in initial_feature_set:
    if col in df_renamed.columns and df_renamed[col].isnull().any():
        new_missing_col_name = f'{col}_is_missing'
        df_renamed[new_missing_col_name] = df_renamed[col].isnull().astype(int)
        if new_missing_col_name not in numeric_cols: # Add to numeric features
            numeric_cols.append(new_missing_col_name)
        missing_indicator_cols.append(new_missing_col_name)
        print(f"    - Added '{new_missing_col_name}'.")
print(f"  ‚úÖ Total {len(missing_indicator_cols)} missingness indicator features created.")


# Age Binning
if 'age' in df_renamed.columns:
    df_renamed['age_bin'] = pd.cut(df_renamed['age'],
                                   bins=[0, 25, 35, 45, 55, np.inf],
                                   labels=['<25', '25-34', '35-44', '45-54', '55+'],
                                   right=False)
    if 'age_bin' not in categorical_cols: categorical_cols.append('age_bin')
    print("  - 'age' has been binned into 'age_bin'.")

# Pin Code Frequency (as a proxy for density)
if 'pin code' in df_renamed.columns:
    df_renamed['pin code'] = df_renamed['pin code'].astype(str)
    pin_code_counts = df_renamed['pin code'].value_counts()
    df_renamed['pin_code_frequency'] = df_renamed['pin code'].map(pin_code_counts)
    if 'pin_code_frequency' not in numeric_cols: numeric_cols.append('pin_code_frequency')
    print("  - 'pin_code_frequency' derived from 'pin code'.")

# --- Aggregations for various financial categories ---
credit_limit_pattern = r'credit_limit_\d+'
balance_pattern = r'balance_\d+'
emi_pattern = r'total_emi_\d+'
inquiries_pattern = r'total_inquiries_\d+|total_inquires_\d+' # Handles both spellings
loan_amt_pattern = r'loan_amt_\d+'
repayment_pattern = r'repayment_\d+' # For repayment consistency

# Helper to find and sum/mean columns
def create_agg_feature(df, pattern, feature_name_sum, feature_name_mean, numeric_cols_list):
    cols = [col for col in df.columns if pd.Series([col]).str.contains(pattern, regex=True, na=False).any() and col in numeric_cols_list]
    if cols:
        df[feature_name_sum] = df[cols].sum(axis=1)
        if feature_name_sum not in numeric_cols_list:
            numeric_cols_list.append(feature_name_sum)

        df[feature_name_mean] = df[cols].mean(axis=1)
        if feature_name_mean not in numeric_cols_list:
            numeric_cols_list.append(feature_name_mean)
        print(f"  - '{feature_name_sum}' and '{feature_name_mean}' aggregated from {len(cols)} columns.")
    return df, numeric_cols_list

df_renamed, numeric_cols = create_agg_feature(df_renamed, credit_limit_pattern, 'total_credit_limit_overall', 'avg_credit_limit_overall', numeric_cols)
df_renamed, numeric_cols = create_agg_feature(df_renamed, balance_pattern, 'total_balance_overall', 'avg_balance_overall', numeric_cols)
df_renamed, numeric_cols = create_agg_feature(df_renamed, emi_pattern, 'total_emi_overall', 'avg_emi_overall', numeric_cols)
df_renamed, numeric_cols = create_agg_feature(df_renamed, inquiries_pattern, 'total_inquiries_overall', 'avg_inquiries_overall', numeric_cols)
df_renamed, numeric_cols = create_agg_feature(df_renamed, loan_amt_pattern, 'total_loan_amount_overall', 'avg_loan_amount_overall', numeric_cols)

# Calculate repayment consistency (using std dev, inverse for score)
repayment_cols_current = [col for col in df_renamed.columns if pd.Series([col]).str.contains(repayment_pattern, regex=True, na=False).any() and col in numeric_cols]
if repayment_cols_current:
    df_renamed['avg_repayment'] = df_renamed[repayment_cols_current].mean(axis=1)
    if 'avg_repayment' not in numeric_cols: numeric_cols.append('avg_repayment')
    # Add 1e-6 to denominator to avoid division by zero when std is 0 (perfect consistency)
    df_renamed['repayment_consistency_score'] = 1 / (df_renamed[repayment_cols_current].std(axis=1).fillna(0) + 1e-6)
    if 'repayment_consistency_score' not in numeric_cols: numeric_cols.append('repayment_consistency_score')
    print("  - 'avg_repayment' and 'repayment_consistency_score' calculated.")


# --- Enhanced Financial Ratios & Interaction Features ---

# Credit Utilization Ratios (enhanced)
if 'total_balance_overall' in df_renamed.columns and 'total_credit_limit_overall' in df_renamed.columns:
    # Overall credit utilization based on aggregated sums
    df_renamed['overall_credit_util_ratio'] = df_renamed['total_balance_overall'] / (df_renamed['total_credit_limit_overall'] + 1e-6)
    df_renamed['overall_credit_util_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
    if 'overall_credit_util_ratio' not in numeric_cols: numeric_cols.append('overall_credit_util_ratio')
    print("  - 'overall_credit_util_ratio' calculated.")

    # Individual credit utilization for specific balance/limit pairs if available
    for i in range(1, 4): # Assuming balance_1/credit_limit_1, etc.
        bal_col = f'balance_{i}'
        lim_col = f'credit_limit_{i}'
        ratio_col = f'credit_util_ratio_{i}'
        if bal_col in df_renamed.columns and lim_col in df_renamed.columns and bal_col in numeric_cols and lim_col in numeric_cols:
            df_renamed[ratio_col] = df_renamed[bal_col] / (df_renamed[lim_col] + 1e-6)
            df_renamed[ratio_col].replace([np.inf, -np.inf], np.nan, inplace=True)
            if ratio_col not in numeric_cols: numeric_cols.append(ratio_col)
            print(f"  - '{ratio_col}' calculated.")


# Income and Debt Ratios (using original scale target for meaningful ratios)
if 'total_emi_overall' in df_renamed.columns and 'target_original_scale' in df_renamed.columns:
    df_renamed['debt_to_income_ratio'] = df_renamed['total_emi_overall'] / (df_renamed['target_original_scale'] + 1e-6)
    df_renamed['debt_to_income_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
    if 'debt_to_income_ratio' not in numeric_cols: numeric_cols.append('debt_to_income_ratio')
    print("  - 'debt_to_income_ratio' calculated.")

    df_renamed['income_to_emi_ratio'] = df_renamed['target_original_scale'] / (df_renamed['total_emi_overall'] + 1e-6)
    df_renamed['income_to_emi_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
    if 'income_to_emi_ratio' not in numeric_cols: numeric_cols.append('income_to_emi_ratio')
    print("  - 'income_to_emi_ratio' calculated.")

if 'total_loan_amount_overall' in df_renamed.columns and 'target_original_scale' in df_renamed.columns:
    df_renamed['income_to_loan_ratio'] = df_renamed['target_original_scale'] / (df_renamed['total_loan_amount_overall'] + 1e-6)
    df_renamed['income_to_loan_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
    if 'income_to_loan_ratio' not in numeric_cols: numeric_cols.append('income_to_loan_ratio')
    print("  - 'income_to_loan_ratio' calculated.")

if 'avg_repayment' in df_renamed.columns and 'target_original_scale' in df_renamed.columns:
    df_renamed['payment_to_income_ratio'] = df_renamed['avg_repayment'] / (df_renamed['target_original_scale'] + 1e-6)
    df_renamed['payment_to_income_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
    if 'payment_to_income_ratio' not in numeric_cols: numeric_cols.append('payment_to_income_ratio')
    print("  - 'payment_to_income_ratio' calculated.")


# Inquiry Ratios
if 'total_emi_overall' in df_renamed.columns and 'total_inquiries_overall' in df_renamed.columns:
    df_renamed['emi_per_inquiry_ratio'] = df_renamed['total_emi_overall'] / (df_renamed['total_inquiries_overall'] + 1e-6)
    df_renamed['emi_per_inquiry_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
    if 'emi_per_inquiry_ratio' not in numeric_cols: numeric_cols.append('emi_per_inquiry_ratio')
    print("  - 'emi_per_inquiry_ratio' calculated.")

if 'total_loan_amount_overall' in df_renamed.columns and 'total_inquiries_overall' in df_renamed.columns:
    df_renamed['loan_amount_per_inquiry_ratio'] = df_renamed['total_loan_amount_overall'] / (df_renamed['total_inquiries_overall'] + 1e-6)
    df_renamed['loan_amount_per_inquiry_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
    if 'loan_amount_per_inquiry_ratio' not in numeric_cols: numeric_cols.append('loan_amount_per_inquiry_ratio')
    print("  - 'loan_amount_per_inquiry_ratio' calculated.")


# Interaction features between numeric columns (selected based on potential domain relevance)
if 'age' in df_renamed.columns and 'total_inquiries_overall' in df_renamed.columns:
    df_renamed['age_x_total_inquiries_overall'] = df_renamed['age'] * df_renamed['total_inquiries_overall']
    if 'age_x_total_inquiries_overall' not in numeric_cols: numeric_cols.append('age_x_total_inquiries_overall')
    print("  - 'age_x_total_inquiries_overall' calculated.")

if 'credit_score' in df_renamed.columns and 'total_emi_overall' in df_renamed.columns:
    df_renamed['total_emi_overall_x_credit_score'] = df_renamed['total_emi_overall'] * df_renamed['credit_score']
    if 'total_emi_overall_x_credit_score' not in numeric_cols: numeric_cols.append('total_emi_overall_x_credit_score')
    print("  - 'total_emi_overall_x_credit_score' calculated.")


# --- Simulate External Data Features (placeholders for real integration) ---
# These are still placeholders, but their presence allows the model to "expect" them
# and for you to understand how to integrate real data later.
if 'night_light_intensity' not in numeric_cols:
    df_renamed['night_light_intensity'] = np.random.rand(len(df_renamed)) * 100
    numeric_cols.append('night_light_intensity')
    print("  - Simulated 'night_light_intensity' added.")
if 'market_density_score' not in numeric_cols:
    df_renamed['market_density_score'] = np.random.rand(len(df_renamed))
    numeric_cols.append('market_density_score')
    print("  - Simulated 'market_density_score' added.")
if 'local_literacy_rate' not in numeric_cols:
    df_renamed['local_literacy_rate'] = np.random.rand(len(df_renamed)) * 100
    numeric_cols.append('local_literacy_rate')
    print("  - Simulated 'local_literacy_rate' added.")

# Simulate Digital Financial Behavior features
if 'upi_transaction_frequency' not in numeric_cols:
    df_renamed['upi_transaction_frequency'] = np.random.randint(0, 50, len(df_renamed))
    numeric_cols.append('upi_transaction_frequency')
    print("  - Simulated 'upi_transaction_frequency' added.")
if 'mobile_recharge_frequency' not in numeric_cols:
    df_renamed['mobile_recharge_frequency'] = np.random.randint(0, 10, len(df_renamed))
    numeric_cols.append('mobile_recharge_frequency')
    print("  - Simulated 'mobile_recharge_frequency' added.")
if 'bill_payment_consistency' not in numeric_cols:
    df_renamed['bill_payment_consistency'] = np.random.rand(len(df_renamed))
    numeric_cols.append('bill_payment_consistency')
    print("  - Simulated 'bill_payment_consistency' added.")

# Simulate Consumption Patterns features
if 'ecommerce_purchase_frequency' not in numeric_cols:
    df_renamed['ecommerce_purchase_frequency'] = np.random.randint(0, 20, len(df_renamed))
    numeric_cols.append('ecommerce_purchase_frequency')
    print("  - Simulated 'ecommerce_purchase_frequency' added.")
if 'digital_service_subscriptions' not in numeric_cols:
    df_renamed['digital_service_subscriptions'] = np.random.randint(0, 5, len(df_renamed))
    numeric_cols.append('digital_service_subscriptions')
    print("  - Simulated 'digital_service_subscriptions' added.")

# Simulate Technology Adoption features
if 'app_diversity_count' not in numeric_cols:
    df_renamed['app_diversity_count'] = np.random.randint(5, 50, len(df_renamed))
    numeric_cols.append('app_diversity_count')
    print("  - Simulated 'app_diversity_count' added.")
if 'digital_literacy_score' not in numeric_cols:
    df_renamed['digital_literacy_score'] = np.random.rand(len(df_renamed))
    numeric_cols.append('digital_literacy_score')
    print("  - Simulated 'digital_literacy_score' added.")


# --- Update feature lists after engineering ---
# Re-identify numeric and categorical columns based on the new df_renamed
# Ensure 'target_original_scale' is *not* a feature
all_numeric_cols_updated = df_renamed.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [col for col in all_numeric_cols_updated if col not in [target_col, 'target_original_scale']]

all_categorical_cols_updated = df_renamed.select_dtypes(include=['object', 'category']).columns.tolist()
# 'pin code' removed from categorical_cols entirely as its frequency is now a numeric feature
id_like_cols_to_remove = ['unique_id', 'score_comments', 'pin code'] # Removed 'pin code' as it's now numeric_frequency
categorical_features_final = [col for col in all_categorical_cols_updated if col not in id_like_cols_to_remove]

print(f"\n‚úÖ New features added and lists updated. Total numeric columns now: {len(numeric_cols)}")
print(f"Total categorical columns now: {len(categorical_cols)}")


# --- 6. Feature Selection based on Missing Values and Correlation (Re-evaluate with new features) ---
# Ensure only features that actually exist in the DataFrame for missing_percent and correlation calculation
features_for_selection = [col for col in numeric_cols + categorical_features_final + [target_col] if col in df_renamed.columns]
missing_percent = df_renamed[features_for_selection].isnull().mean().sort_values(ascending=False)
print("\nüï≥Ô∏è Top 10 columns with missing values (after feature engineering):")
print(missing_percent.head(10))

print("\nüìâ Columns with >50% missing (after feature engineering):")
high_missing_cols = missing_percent[missing_percent > 0.5].index.tolist()
print(high_missing_cols)

# Compute correlation with target for numeric columns
corr_with_target = {}
for col in numeric_cols:
    subset_df = df_renamed[[col, target_col]].dropna()
    if not subset_df.empty:
        corr = subset_df[col].corr(subset_df[target_col])
        corr_with_target[col] = abs(corr) if not np.isnan(corr) else 0.0
    else:
        corr_with_target[col] = 0.0

correlation = pd.Series(corr_with_target).sort_values(ascending=False)

print("\nüìä Top 15 features most correlated with target (after feature engineering):")
print(correlation.head(15))

print("\nüîª Bottom 10 least correlated (near zero) (after feature engineering):")
print(correlation.tail(10))

# Define thresholds for dropping features
missing_threshold = 0.60
correlation_threshold = 0.005

features_to_drop_numeric = []
features_to_keep_numeric = []

for col in numeric_cols:
    miss = missing_percent.get(col, 0.0)
    corr = correlation.get(col, 0.0)

    if miss > missing_threshold and corr < correlation_threshold:
        features_to_drop_numeric.append(col)
    else:
        features_to_keep_numeric.append(col)

print(f"\n‚úÖ Keeping {len(features_to_keep_numeric)} numeric features")
print(f"üóëÔ∏è Dropping {len(features_to_drop_numeric)} numeric features due to high missing & low correlation:")
print(features_to_drop_numeric)

print("\nüî§ Categorical Columns to keep for processing:")
print(categorical_features_final)


# --- 7. Define Preprocessing Pipelines ---

# Target Encoding for selected categorical features (applied before other preprocessing)
target_encode_cols = ['city', 'state', 'marital_status', 'gender', 'residence_ownership', 'age_bin']
# Ensure these are in the final categorical features list before encoding
target_encode_cols = [col for col in target_encode_cols if col in categorical_features_final]


# Remove target_encoded cols from categorical_features_final for OneHotEncoder
# They will become numeric after target encoding
categorical_features_for_onehot = [col for col in categorical_features_final if col not in target_encode_cols]


# --- 8. Prepare X and y (with Target Encoding applied early) ---
# First, separate features and target *before* target encoding
# Use all columns that will eventually be processed, including original categorical ones
initial_features_for_X = [col for col in (numeric_cols + categorical_features_final) if col in df_renamed.columns]
X = df_renamed[initial_features_for_X].copy()
y = df_renamed[target_col].copy()


# Drop rows where target is NaN (if any, after transformation)
if y.isnull().any():
    print(f"Warning: Target column '{target_col}' contains NaN values. These rows will be dropped for training.")
    nan_target_rows = y.isnull()
    X = X[~nan_target_rows]
    y = y[~nan_target_rows]
    print(f"Dropped {nan_target_rows.sum()} rows due to NaN in target. New data shape: {X.shape}")


# Apply Target Encoding
encoder = ce.TargetEncoder(cols=target_encode_cols, smoothing=0.2, min_samples_leaf=20)

# Fit encoder on the whole X,y for simplicity in this script.
# In a rigorous CV setup, this would be done on training folds only.
X_encoded = encoder.fit_transform(X, y)

# After target encoding, update features_to_keep_numeric with the new target-encoded features
# and remove them from categorical_features_for_onehot
# This ensures they are treated as numeric in the preprocessor.
for col in target_encode_cols:
    if col not in features_to_keep_numeric: # Add if not already numeric
        features_to_keep_numeric.append(col)
    if col in categorical_features_for_onehot: # Remove from one-hot list
        categorical_features_for_onehot.remove(col)


# Redefine the ColumnTransformer with the updated feature lists
numeric_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_pipeline, features_to_keep_numeric), # These now include target-encoded features
        ('cat', categorical_pipeline, categorical_features_for_onehot) # Remaining categoricals for OneHot
    ],
    remainder='drop'
)

# Fit and transform the features
X_processed = preprocessor.fit_transform(X_encoded) # Fit on the target-encoded X
print("‚úÖ Preprocessing complete. Final Feature Matrix Shape:", X_processed.shape)


# --- 9. Split Data for Training and Evaluation ---
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X_processed, y, test_size=0.2, random_state=42
)

X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.25, random_state=42
)

print(f"\nTraining set shape: {X_train.shape}")
print(f"Validation set shape: {X_val.shape}")
print(f"Test set shape: {X_test.shape}")

# --- Helper function for evaluating model performance ---
def evaluate_model_performance(model_name, y_true_transformed, y_pred_transformed, y_true_original_scale):
    y_pred_original_scale = np.expm1(y_pred_transformed)
    mse = mean_squared_error(y_true_original_scale, y_pred_original_scale)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true_original_scale, y_pred_original_scale)
    r2 = r2_score(y_true_original_scale, y_pred_original_scale)

    print(f"\n--- {model_name} Performance ---")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"R-squared (R2): {r2:.4f}")
    return {"MSE": mse, "RMSE": rmse, "MAE": mae, "R2": r2}


# --- 10. Hyperparameter Optimization with Optuna for Each Base Model ---
print("\nüìä Starting Hyperparameter Optimization for individual models with Optuna (this may take significant time)...")

# --- Optuna Objective for XGBoost ---
def objective_xgb(trial):
    param = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'max_depth': trial.suggest_int('max_depth', 3, 6),
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'gamma': 0,
        'reg_alpha': 0,
        'min_child_weight': 1,
        'random_state': 42,
    }
    model = XGBRegressor(**param)
    model.fit(X_train, y_train)
    y_val_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    return rmse

print("\n--- Optimizing XGBoost Regressor with Optuna ---")
study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
study_xgb.optimize(objective_xgb, n_trials=10, show_progress_bar=True)  # Reduced from 50 to 10 trials
print(f"‚úÖ XGBoost Optuna Best RMSE: {study_xgb.best_value:.4f}")
print("XGBoost Best parameters:", study_xgb.best_params)


# --- Optuna Objective for CatBoost ---
def objective_cat(trial):
    param = {
        'iterations': trial.suggest_int('iterations', 100, 500, step=100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'depth': trial.suggest_int('depth', 3, 6),
        'l2_leaf_reg': 3,
        'loss_function': 'RMSE',
        'eval_metric': 'RMSE',
        'random_seed': 42,
        'verbose': 0,
        'early_stopping_rounds': 50
    }
    model = CatBoostRegressor(**param)
    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)
    return model.get_best_score()['validation']['RMSE']

print("\n--- Optimizing CatBoost Regressor with Optuna ---")
study_cat = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
study_cat.optimize(objective_cat, n_trials=10, show_progress_bar=True)  # Reduced from 50 to 10 trials
print(f"‚úÖ CatBoost Optuna Best RMSE: {study_cat.best_value:.4f}")
print("CatBoost Best parameters:", study_cat.best_params)


# --- Optuna Objective for RandomForest ---
def objective_rf(trial):
    param = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=100),
        'max_depth': trial.suggest_int('max_depth', 3, 6),
        'min_samples_split': 2,
        'min_samples_leaf': 1,
        'max_features': 0.8,
        'random_state': 42,
        'n_jobs': -1
    }
    model = RandomForestRegressor(**param)
    model.fit(X_train, y_train)
    y_val_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    return rmse

print("\n--- Optimizing RandomForest Regressor with Optuna ---")
study_rf = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
study_rf.optimize(objective_rf, n_trials=10, show_progress_bar=True)  # Reduced from 30 to 10 trials
print(f"‚úÖ RandomForest Optuna Best RMSE: {study_rf.best_value:.4f}")
print("RandomForest Best parameters:", study_rf.best_params)


# --- 11. Train Final Individual Models with Optimized Parameters ---
print("\nüéØ Training final individual models with optimized parameters (for ensemble and SHAP)...")

# XGBoost
final_xgb_model = XGBRegressor(**study_xgb.best_params)
final_xgb_model.set_params(objective='reg:squarederror', eval_metric='rmse', device='cuda', predictor='gpu_predictor', random_state=42)
final_xgb_model.fit(X_trainval, y_trainval) # Train on combined train+val for final model
print("‚úÖ Final XGBoost model trained with Optuna-tuned parameters.")

# CatBoost
final_cat_model = CatBoostRegressor(**study_cat.best_params)
final_cat_model.set_params(loss_function='RMSE', eval_metric='RMSE', random_seed=42, verbose=0)
final_cat_model.fit(X_trainval, y_trainval) # Train on combined train+val for final model
print("‚úÖ Final CatBoost model trained with Optuna-tuned parameters.")

# RandomForest
final_rf_model = RandomForestRegressor(**study_rf.best_params)
final_rf_model.set_params(random_state=42, n_jobs=-1)
final_rf_model.fit(X_trainval, y_trainval) # Train on combined train+val for final model
print("‚úÖ Final RandomForest model trained with Optuna-tuned parameters.")


# --- 12. Create and Train Ensemble Model (VotingRegressor) ---
print("\nüéØ Creating and training ensemble model with optimized base models...")

ensemble_model = VotingRegressor(estimators=[
    ('xgb', final_xgb_model),
    ('cat', final_cat_model),
    ('rf', final_rf_model)
], weights=[0.4, 0.4, 0.2], n_jobs=-1) # Weights can be further tuned

# VotingRegressor does not need to re-fit if estimators are already fitted
# The 'fit' method of VotingRegressor will internally call fit on its estimators if they are not already fitted.
# Since we have explicitly fitted them above on X_trainval, this call should be quick and combine them.
ensemble_model.fit(X_trainval, y_trainval)
print("‚úÖ Ensemble model training complete.")


# --- 13. Evaluate Ensemble Model Performance on Test Set ---
y_pred_transformed_ensemble = ensemble_model.predict(X_test)
y_pred_ensemble = np.expm1(y_pred_transformed_ensemble)
y_test_original = np.expm1(y_test)

mse_ensemble = mean_squared_error(y_test_original, y_pred_ensemble)
rmse_ensemble = np.sqrt(mse_ensemble)
mae_ensemble = mean_absolute_error(y_test_original, y_pred_ensemble)
r2_ensemble = r2_score(y_test_original, y_pred_ensemble)

print("\n--- Ensemble Model Evaluation on Test Set (Original Scale) ---")
print(f"Mean Squared Error (MSE): {mse_ensemble:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse_ensemble:.4f}")
print(f"Mean Absolute Error (MAE): {mae_ensemble:.4f}")
print(f"R-squared (R2): {r2_ensemble:.4f}")


# --- 14. Model Explainability with SHAP (for XGBoost as primary insight) ---
print("\nüîç Generating SHAP values for XGBoost model in the ensemble for explainability...")

try:
    feature_names = preprocessor.get_feature_names_out()
    # Use the FINAL trained XGBoost model (final_xgb_model) for SHAP
    explainer = shap.TreeExplainer(final_xgb_model)

    shap_values = explainer.shap_values(X_test)

    print("\nVisualizing SHAP Summary Plot for XGBoost...")
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False, plot_type="dot")
    plt.tight_layout()
    plt.show()
    print("‚úÖ SHAP Summary Plot generated for XGBoost.")

except Exception as e:
    print(f"‚ö†Ô∏è Error generating SHAP plots for XGBoost: {e}")
    print("Ensure 'shap' library is installed (`!pip install shap`) and output is a dense array if needed.")


# --- Optional: Save Ensemble Model and Preprocessor ---
try:
    with open("ensemble_model.pkl", "wb") as f:
        pickle.dump(ensemble_model, f)
    print("\n‚úÖ Ensemble model saved as ensemble_model.pkl")

    # Save the preprocessor using joblib (more robust for sklearn objects)
    joblib.dump(preprocessor, "preprocessor.joblib")
    print("‚úÖ Preprocessor saved as preprocessor.joblib")
except Exception as e:
    print(f"Error saving model or preprocessor: {e}")


complete_data = {
    'age': 35,
    'gender': 'M',
    'marital_status': 'Married',
    'city': 'Mumbai',
    'state': 'Maharashtra',
    'residence_ownership': 'Owned',
    'credit_score': 750,
    'credit_limit_1': 50000,
    'balance_1': 25000,
    'total_emi_1': 5000,
    'total_inquiries_1': 2,
    'loan_amt_1': 200000,
    'repayment_1': 4500,
    'digital_literacy_score': 0.85,
    'ecommerce_purchase_frequency': 10,
    'digital_service_subscriptions': 3,
    'upi_transaction_frequency': 25,
    'total_loan_recent_is_missing': 0,
    'market_density_score': 0.8,
    'app_diversity_count': 25,
    'mobile_recharge_frequency': 5,
    'bill_payment_consistency': 0.9,
    'night_light_intensity': 75.5,
    'local_literacy_rate': 85.0,
    'total_loan_recent': 0  # or a realistic value
}

